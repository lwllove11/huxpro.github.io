---
layout:     post
title:      "tensorflow 梯度下降法详解"
subtitle:   "梯度下降法"
date:       2018-10-1 12:00:00
author:     "Idisfkj"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 机器学习
    - python

---

> “This is my world. ”

### 梯度下降的概念

------

`梯度下降法`是一个一阶最优化算法，通常也称为最速下降法。要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对于梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。所以梯度下降法可以帮助我们求解某个函数的极小值或者最小值。对于n维问题就最优解，梯度下降法是最常用的方法之一。下面通过梯度下降法的`前生今世`来进行详细推导说明。

### 梯度下降法的前世

------

首先从简单的开始，看下面的一维函数：

```
f(x) = x^3 + 2 * x - 3
```

在数学中如果我们要求`f(x) = 0`处的解，我们可以通过如下误差等式来求得：

```
error = (f(x) - 0)^2
```

当`error`趋近于最小值时，也就是`f(x) = 0`处`x`的解，我们也可以通过图来观察：

通过这函数图，我们可以非常直观的发现，要想求得该函数的最小值，只要将`x`指定为函数图的最低谷。这在高中我们就已经掌握了该函数的最小值解法。我们可以通过对该函数进行求导（即斜率）：

```
derivative(x) = 6 * x^5 + 16 * x^3 - 18 * x^2 + 8 * x - 12
```

如果要得到最小值，只需令`derivative(x) = 0`，即`x = 1`。同时我们结合图与导函数可以知道：

- 当`x < 1`时，`derivative < 0`，斜率为负的；
- 当`x > 1`时，`derivative > 0`，斜率为正的；
- 当`x 无限接近 1`时，`derivative也就无限=0`，斜率为零。

通过上面的结论，我们可以使用如下表达式来代替`x`在函数中的移动

```
x = x - reate * derivative
```

> *当斜率为负的时候，x增大，当斜率为正的时候，x减小；因此x总是会向着低谷移动，使得error最小，从而求得 f(x) = 0处的解。其中的rate代表x逆着导数方向移动的距离，rate越大，x每次就移动的越多。反之移动的越少。*

这是针对简单的函数，我们可以非常直观的求得它的导函数。为了应对复杂的函数，我们可以通过使用求导函数的定义来表达导函数:若函数`f(x)`在点`x0`处可导，那么有如下定义：

![clipboard.png](https://segmentfault.com/img/bVYuru?w=431&h=54)

上面是都是公式推导，下面通过代码来实现，下面的代码都是使用`python`进行实现。

```
>>> def f(x):
...     return x**3 + 2 * x - 3
...
>>> def error(x):
...     return (f(x) - 0)**2
...
>>> def gradient_descent(x):
...     delta = 0.00000001
...     derivative = (error(x + delta) - error(x)) / delta
...     rate = 0.01
...     return x - rate * derivative
...
>>> x = 0.8
>>> for i in range(50):
...     x = gradient_descent(x)
...     print('x = {:6f}, f(x) = {:6f}'.format(x, f(x)))
...
```

执行上面程序，我们就能得到如下结果：

```
x = 0.999590, f(x) = -0.002048
x = 0.999795, f(x) = -0.001025
x = 0.999897, f(x) = -0.000513
x = 0.999949, f(x) = -0.000256
x = 0.999974, f(x) = -0.000128
x = 0.999987, f(x) = -0.000064
x = 0.999994, f(x) = -0.000032
x = 0.999997, f(x) = -0.000016
x = 0.999998, f(x) = -0.000008
x = 0.999999, f(x) = -0.000004
x = 1.000000, f(x) = -0.000002
x = 1.000000, f(x) = -0.000001
x = 1.000000, f(x) = -0.000001
x = 1.000000, f(x) = -0.000000
x = 1.000000, f(x) = -0.000000
x = 1.000000, f(x) = -0.000000
```

通过上面的结果，也验证了我们最初的结论。`x = 1`时，`f(x) = 0`。
所以通过该方法，只要步数足够多，就能得到非常精确的值。

### 梯度下降法的今生

------

上面是对`一维`函数进行求解，那么对于`多维`函数又要如何求呢？我们接着看下面的函数，你会发现对于`多维`函数也是那么的简单。

```
f(x) = x[0] + 2 * x[1] + 4
```

同样的如果我们要求`f(x) = 0`处，`x[0]`与`x[1]`的值，也可以通过求`error`函数的最小值来间接求`f(x)`的解。跟`一维`函数唯一不同的是，要分别对`x[0]`与`x[1]`进行求导。在数学上叫做`偏导数`：

- 保持x[1]不变，对`x[0]`进行求导，即`f(x)`对`x[0]`的偏导数
- 保持x[0]不变，对`x[1]`进行求导，即`f(x)`对`x[1]`的偏导数

有了上面的理解基础，我们定义的`gradient_descent`如下：

```
>>> def gradient_descent(x):
...     delta = 0.00000001
...     derivative_x0 = (error([x[0] + delta, x[1]]) - error([x[0], x[1]])) / delta
...     derivative_x1 = (error([x[0], x[1] + delta]) - error([x[0], x[1]])) / delta
...     rate = 0.01
...     x[0] = x[0] - rate * derivative_x0
...     x[1] = x[1] - rate * derivative_x1
...     return [x[0], x[1]]
...
```

`rate`的作用不变，唯一的区别就是分别获取最新的`x[0]`与`x[1]`。下面是整个代码：

```
>>> def f(x):
...     return x[0] + 2 * x[1] + 4
...
>>> def error(x):
...     return (f(x) - 0)**2
...
>>> def gradient_descent(x):
...     delta = 0.00000001
...     derivative_x0 = (error([x[0] + delta, x[1]]) - error([x[0], x[1]])) / delta
...     derivative_x1 = (error([x[0], x[1] + delta]) - error([x[0], x[1]])) / delta
...     rate = 0.02
...     x[0] = x[0] - rate * derivative_x0
...     x[1] = x[1] - rate * derivative_x1
...     return [x[0], x[1]]
...
>>> x = [-0.5, -1.0]
>>> for i in range(100):
...     x = gradient_descent(x)
...     print('x = {:6f},{:6f}, f(x) = {:6f}'.format(x[0],x[1],f(x)))
...
```

输出结果为：

```
x = -0.799998,-1.599997, f(x) = 0.000009
x = -0.799999,-1.599997, f(x) = 0.000007
x = -0.799999,-1.599998, f(x) = 0.000006
x = -0.799999,-1.599998, f(x) = 0.000004
x = -0.799999,-1.599999, f(x) = 0.000004
x = -0.799999,-1.599999, f(x) = 0.000003
x = -0.800000,-1.599999, f(x) = 0.000002
x = -0.800000,-1.599999, f(x) = 0.000002
x = -0.800000,-1.599999, f(x) = 0.000001
x = -0.800000,-1.600000, f(x) = 0.000001
x = -0.800000,-1.600000, f(x) = 0.000001
x = -0.800000,-1.600000, f(x) = 0.000001
x = -0.800000,-1.600000, f(x) = 0.000001
x = -0.800000,-1.600000, f(x) = 0.000000
```

细心的你可能会发现，`f(x) = 0`不止这一个解还可以是`x = -2, -1`。这是因为梯度下降法只是对`当前所处的凹谷`进行梯度下降求解，对于`error`函数并不代表只有一个`f(x) = 0`的凹谷。所以梯度下降法只能求得局部解，但不一定能求得全部的解。当然如果对于非常复杂的函数，能够求得局部解也是非常不错的。

### tensorflow中的运用

通过上面的示例，相信对`梯度下降`也有了一个基本的认识。现在我们回到最开始的地方，在`tensorflow`中使用`gradientDescent`。

```
import tensorflow as tf
 
# Model parameters
W = tf.Variable([.3], dtype=tf.float32)
b = tf.Variable([-.3], dtype=tf.float32)
# Model input and output
x = tf.placeholder(tf.float32)
linear_model = W*x + b
y = tf.placeholder(tf.float32)
 
# loss
loss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares
# optimizer
optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss)
 
# training data
x_train = [1, 2, 3, 4]
y_train = [0, -1, -2, -3]
# training loop
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init) # reset values to wrong
for i in range(1000):
  sess.run(train, {x: x_train, y: y_train})
 
# evaluate training accuracy
curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})
print("W: %s b: %s loss: %s"%(curr_W, curr_b, curr_loss))
```

上面的是[tensorflow](https://www.tensorflow.org/get_started/get_started)的官网示例，上面代码定义了函数`linear_model = W * x + b`,其中的`error`函数为`linear_model - y`。目的是对一组`x_train`与`y_train`进行简单的训练求解`W`与`b`。为了求得这一组数据的最优解，将每一组的`error`相加从而得到`loss`，最后再对`loss`进行梯度下降求解最优值。

```
optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss)
```

在这里`rate`为`0.01`,因为这个示例也是`多维`函数，所以也要用到`偏导数`来进行逐步向最优解靠近。

```
for i in range(1000):
  sess.run(train, {x: x_train, y: y_train})
   
```

最后使用`梯度下降`进行循环推导，下面给出一些推导过程中的相关结果

```
W: [-0.21999997] b: [-0.456] loss: 4.01814
W: [-0.39679998] b: [-0.49552] loss: 1.81987
W: [-0.45961601] b: [-0.4965184] loss: 1.54482
W: [-0.48454273] b: [-0.48487374] loss: 1.48251
W: [-0.49684232] b: [-0.46917531] loss: 1.4444
W: [-0.50490189] b: [-0.45227283] loss: 1.4097
W: [-0.5115062] b: [-0.43511063] loss: 1.3761
....
....
....
W: [-0.99999678] b: [ 0.99999058] loss: 5.84635e-11
W: [-0.99999684] b: [ 0.9999907] loss: 5.77707e-11
W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11
```

这里就不推理验证了，如果看了上面的`梯度下降`的前世今生，相信能够自主的推导出来。那么我们直接看最后的结果，可以估算为`W = -1.0`与`b  = 1.0`，将他们带入上面的`loss`得到的结果为`0.0`，即误差损失值最小，所以`W = -1.0`与`b = 1.0`就是`x_train`与`y_train`这组数据的最优解。

<br>
source: <a href="https://idisfkj.github.io/2017/11/06/tensorflow-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-%E6%9C%89%E8%BF%99%E4%B8%80%E7%AF%87%E5%B0%B1%E8%B6%B3%E5%A4%9F%E4%BA%86/">Idisfkj</a>